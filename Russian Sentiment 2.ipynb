{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc151fd0",
   "metadata": {},
   "source": [
    "<h3>Импортируем библиотеки</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b48eff78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/anna/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import json\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import pickle\n",
    "import nltk\n",
    "from sklearn import metrics\n",
    "nltk.download('punkt')\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa23ad84",
   "metadata": {},
   "source": [
    "<h3>Считываем данные</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f43eca10",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_table = pd.read_json('train.json')\n",
    "test_table = pd.read_json('test.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73da9fe4",
   "metadata": {},
   "source": [
    "<h3>Строим модель</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24666912",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ru_token(string):\n",
    "    return [i.lower() for i in word_tokenize(string) if re.match(r'[\\u0400-\\u04ffа́]+$', i)]\n",
    "\n",
    "# Разбиваем нашу строку на подстроки и возвращаем список слов, которые подходят под кириллицу\n",
    "\n",
    "params = {}\n",
    "params['tokenizer'] = ru_token\n",
    "params['stop_words'] = stopwords.words('russian')\n",
    "params['ngram_range'] = (1, 3)\n",
    "params['min_df'] = 3\n",
    "\n",
    "tfidf  = TfidfVectorizer(**params)\n",
    "\n",
    "# Задаем параметры для tf–idf "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6d2c79",
   "metadata": {},
   "source": [
    "Разделить количество употреблений каждого слова в документе на общее количество слов в документе. Этот новый признак называется tf — Частота термина.\n",
    "\n",
    "Следующее уточнение меры tf — это снижение веса слова, которое появляется во многих документах в корпусе, и отсюда является менее информативным, чем те, которые используются только в небольшой части корпуса. Примером низко ифнормативных слов могут служить служебные слова, артикли, предлоги, союзы и т.п.\n",
    "\n",
    "Это снижение называется tf–idf, что значит “Term Frequency times Inverse Document Frequency” (обратная частота термина)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bb0b7c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_38132/3310056640.py:1: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  all_data = train_table.append(test_table, ignore_index=True)\n",
      "/home/anna/my_project_dir/my_project_env/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(min_df=3, ngram_range=(1, 3),\n",
       "                stop_words=['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с',\n",
       "                            'со', 'как', 'а', 'то', 'все', 'она', 'так', 'его',\n",
       "                            'но', 'да', 'ты', 'к', 'у', 'же', 'вы', 'за', 'бы',\n",
       "                            'по', 'только', 'ее', 'мне', ...],\n",
       "                tokenizer=<function ru_token at 0x7fa827dfe8b0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data = train_table.append(test_table, ignore_index=True)\n",
    "tfidf.fit([row['text'].lower() for index, row in all_data.iterrows()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc4dec7",
   "metadata": {},
   "source": [
    "<h3>Разделяем данные</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da82c673",
   "metadata": {},
   "source": [
    "Разделим **train_table** на два словаря, один будет составлять 80% исходных данных, второй 20% данных. Таким образом, это будут обучающие и тестовые данные соотвественно для оценки точности нашей модели.\n",
    "\n",
    "Первый словарь будет в ключах иметь **sentiment** в значениях list из **text**. Затем воспользуемся функцией, **train_test_split** которая \"split arrays or matrices into random train and test subsets.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f2ec9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train1, train2, tmp = defaultdict(list), defaultdict(list), defaultdict(list)\n",
    "for index, row in train_table.iterrows():\n",
    "    tmp[row['sentiment']].append(row['text'].lower())\n",
    "for element in tmp:\n",
    "    train1[element], train2[element] = train_test_split(tmp[element], test_size=0.2, random_state=2022)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca62ac6",
   "metadata": {},
   "source": [
    "[Здесь](https://www.analyticsvidhya.com/blog/2020/07/10-techniques-to-deal-with-class-imbalance-in-machine-learning/) можно почить о различных техниках по выравниваю данных и для чего вообще это нужно. Мы будем использовать Random Over-Sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc3f91ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align(some_dict):\n",
    "    rand = np.random.RandomState(2022)\n",
    "    \n",
    "    max_len = 0\n",
    "    \n",
    "    for key in some_dict:\n",
    "        max_len = max(max_len, len(some_dict[key]))\n",
    "    print('max_len: ' + str(max_len))\n",
    "  \n",
    "    tmp = defaultdict(list)\n",
    "    for key in some_dict:\n",
    "        if len(some_dict[key]) < max_len:\n",
    "            \n",
    "            _tmp = some_dict[key].copy()\n",
    "            rand.shuffle(_tmp)\n",
    "            \n",
    "            tmp[key] = some_dict[key] * (max_len // len(some_dict[key])) + _tmp[:(max_len % len(some_dict[key]))]\n",
    "            rand.shuffle(tmp[key])\n",
    "        else:\n",
    "            tmp[key] = some_dict[key]\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7cf4809b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_len: 3227\n"
     ]
    }
   ],
   "source": [
    "train1 = align(train1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bb4749",
   "metadata": {},
   "source": [
    "А теперь сформируем два массива **train_x** с текстами, **train_y** с их оценками. Посортируем ключи, чтобы сначала шли все тексты одного ключа, потом второго, затем тертьего и обучимся."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff533d3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x = []\n",
    "for i in sorted(train1.keys()):\n",
    "    for j in train1[i]:\n",
    "        train_x.append(j)\n",
    "        \n",
    "train_y = []\n",
    "for i in sorted(train1.keys()):\n",
    "    for j in train1[i]:\n",
    "        train_y.append(i)\n",
    "        \n",
    "softmax = MultinomialNB()\n",
    "softmax.fit(tfidf.transform(train_x), train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92dbd165",
   "metadata": {},
   "source": [
    "А теперь аналогично сформируем два массива **test_x** с текстами, **true** с их оценками. Посортируем ключи, чтобы сначала шли все тексты одного ключа, потом второго, затем тертьего. Сделаем предсказания и оценим насколько точно мы их сделали."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a63e176f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = []\n",
    "for i in sorted(train2.keys()):\n",
    "    for j in train2[i]:\n",
    "        test_x.append(j)\n",
    "        \n",
    "true = []\n",
    "for i in sorted(train2.keys()):\n",
    "    for j in train2[i]:\n",
    "        true.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72af5a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = softmax.predict(tfidf.transform(test_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5facc01f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6981246218995766"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(true, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bdf887",
   "metadata": {},
   "source": [
    "<h3>Отправляем результаты</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d685b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_pred = softmax.predict(tfidf.transform([row['text'].lower() for index, row in test_table.iterrows()]))\n",
    "sub_df = pd.DataFrame()\n",
    "sub_df['id'] =  [row['id'] for index, row in test_table.iterrows()]\n",
    "sub_df['sentiment'] = sub_pred\n",
    "\n",
    "sub_df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce4fbce",
   "metadata": {},
   "source": [
    "<h3>Сохраняем модель для бота</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8a761dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('NLP2.pkl', 'wb+') as f:\n",
    "    pickle.dump((tfidf, softmax), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a55fb04",
   "metadata": {},
   "source": [
    "*Обучащие и тестовые данные взяты [отсюда](https://www.kaggle.com/competitions/sentiment-analysis-in-russian/overview)*.\n",
    "\n",
    "*Файл с моделью вот [тут](https://disk.yandex.ru/d/KQPaKH7sBw0hvA)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567b227c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
